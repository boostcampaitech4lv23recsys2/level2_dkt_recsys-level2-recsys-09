{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch_geometric.nn.models import LightGCN\n",
    "\n",
    "from src.datasets import prepare_dataset   # lightgcn\n",
    "from src.utils import class2dict, get_logger   # lightgcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    use_cuda_if_available = True\n",
    "    user_wandb = True\n",
    "    wandb_kwargs = dict(project=\"dkt-gcn\")\n",
    "\n",
    "    # data\n",
    "    basepath = \"/opt/ml/workspace/kch_dkt/data/\"\n",
    "    loader_verbose = True\n",
    "\n",
    "    # dump\n",
    "    output_dir = \"/opt/ml/workspace/kch_dkt/code/lightgcn/output\"\n",
    "    pred_file = \"submission.csv\"\n",
    "\n",
    "    # build\n",
    "    embedding_dim = 64  # 64\n",
    "    num_layers = 6  # 1\n",
    "    alpha = None  # Optional[Union[float, Tensor]]\n",
    "    build_kwargs = {}  # other arguments\n",
    "    weight = \"./weight/best_model.pt\"\n",
    "\n",
    "    # train\n",
    "    n_epoch = 30    # 20\n",
    "    learning_rate = 0.0005   # 0.001\n",
    "    weight_basepath = \"./weight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_conf = {  # only used when 'user_wandb==False'\n",
    "    \"version\": 1,\n",
    "    \"formatters\": {\n",
    "        \"basic\": {\"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n",
    "    },\n",
    "    \"handlers\": {\n",
    "        \"console\": {\n",
    "            \"class\": \"logging.StreamHandler\",\n",
    "            \"level\": \"INFO\",\n",
    "            \"formatter\": \"basic\",\n",
    "            \"stream\": \"ext://sys.stdout\",\n",
    "        },\n",
    "        \"file_handler\": {\n",
    "            \"class\": \"logging.FileHandler\",\n",
    "            \"level\": \"DEBUG\",\n",
    "            \"formatter\": \"basic\",\n",
    "            \"filename\": \"run.log\",\n",
    "        },\n",
    "    },\n",
    "    \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\", \"file_handler\"]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-08 05:46:40,744 - data - INFO - Train Dataset Info\n",
      "2022-12-08 05:46:40,745 - data - INFO -  * Num. Users    : 7442\n",
      "2022-12-08 05:46:40,746 - data - INFO -  * Max. UserID   : 7441\n",
      "2022-12-08 05:46:40,747 - data - INFO -  * Num. Items    : 9454\n",
      "2022-12-08 05:46:40,747 - data - INFO -  * Num. Records  : 2475962\n",
      "2022-12-08 05:46:40,748 - data - INFO - Test Dataset Info\n",
      "2022-12-08 05:46:40,749 - data - INFO -  * Num. Users    : 744\n",
      "2022-12-08 05:46:40,750 - data - INFO -  * Max. UserID   : 7439\n",
      "2022-12-08 05:46:40,750 - data - INFO -  * Num. Items    : 444\n",
      "2022-12-08 05:46:40,751 - data - INFO -  * Num. Records  : 744\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(logging_conf)\n",
    "use_cuda = torch.cuda.is_available() and CFG.use_cuda_if_available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_data, test_data, n_node = prepare_dataset(\n",
    "        device, CFG.basepath, verbose=CFG.loader_verbose, logger=logger.getChild(\"data\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif CFG.user_wandb:\\n    import wandb\\n\\n    wandb.init(**CFG.wandb_kwargs, config=class2dict(CFG))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if CFG.user_wandb:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(**CFG.wandb_kwargs, config=class2dict(CFG))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(n_node, weight=None, logger=None, **kwargs):\n",
    "    model = LightGCN(n_node, **kwargs)\n",
    "    if weight:\n",
    "        if not os.path.isfile(weight):\n",
    "            logger.fatal(\"Model Weight File Not Exist\")\n",
    "        logger.info(\"Load model\")\n",
    "        state = torch.load(weight)[\"model\"]\n",
    "        model.load_state_dict(state)\n",
    "        return model\n",
    "    else:\n",
    "        logger.info(\"No load model\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-08 05:46:41,030 - build - INFO - No load model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGCN(16896, 64, num_layers=6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build(\n",
    "        n_node,\n",
    "        embedding_dim=CFG.embedding_dim,\n",
    "        num_layers=CFG.num_layers,\n",
    "        alpha=CFG.alpha,\n",
    "        logger=logger.getChild(\"build\"),\n",
    "        **CFG.build_kwargs\n",
    "    )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif CFG.user_wandb:\\n        wandb.watch(model)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if CFG.user_wandb:\n",
    "        wandb.watch(model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_data,\n",
    "    valid_data=None,\n",
    "    n_epoch=100,\n",
    "    learning_rate=0.01,\n",
    "    use_wandb=False,\n",
    "    weight=None,\n",
    "    logger=None,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if not os.path.exists(weight):\n",
    "        os.makedirs(weight)\n",
    "\n",
    "    if valid_data is None:\n",
    "        eids = np.arange(len(train_data[\"label\"]))\n",
    "        eids = np.random.permutation(eids)[:1000]\n",
    "        edge, label = train_data[\"edge\"], train_data[\"label\"]\n",
    "        label = label.to(\"cpu\").detach().numpy()\n",
    "        valid_data = dict(edge=edge[:, eids], label=label[eids])\n",
    "\n",
    "    logger.info(f\"Training Started : n_epoch={n_epoch}\")\n",
    "    best_auc, best_epoch = 0, -1\n",
    "    for e in range(n_epoch):\n",
    "        # forward\n",
    "        pred = model(train_data[\"edge\"])\n",
    "        loss = model.link_pred_loss(pred, train_data[\"label\"])\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prob = model.predict_link(valid_data[\"edge\"], prob=True)\n",
    "            prob = prob.detach().cpu().numpy()\n",
    "            acc = accuracy_score(valid_data[\"label\"], prob > 0.5)\n",
    "            auc = roc_auc_score(valid_data[\"label\"], prob)\n",
    "            logger.info(\n",
    "                f\" * In epoch {(e+1):04}, loss={loss:.03f}, acc={acc:.03f}, AUC={auc:.03f}\"\n",
    "            )\n",
    "\n",
    "            \"\"\"\n",
    "            if use_wandb:\n",
    "                import wandb\n",
    "\n",
    "                wandb.log(dict(loss=loss, acc=acc, auc=auc))\n",
    "            \"\"\"\n",
    "\n",
    "        if weight:\n",
    "            if auc > best_auc:\n",
    "                logger.info(\n",
    "                    f\" * In epoch {(e+1):04}, loss={loss:.03f}, acc={acc:.03f}, AUC={auc:.03f}, Best AUC\"\n",
    "                )\n",
    "                best_auc, best_epoch = auc, e\n",
    "                \"\"\"\n",
    "                torch.save(\n",
    "                    {\"model\": model.state_dict(), \"epoch\": e + 1},\n",
    "                    os.path.join(weight, f\"best_model.pt\"),\n",
    "                )\n",
    "    torch.save(\n",
    "        {\"model\": model.state_dict(), \"epoch\": e + 1},\n",
    "        os.path.join(weight, f\"last_model.pt\"),\n",
    "    )\n",
    "    \"\"\"\n",
    "    logger.info(f\"Best Weight Confirmed : {best_epoch+1}'th epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-08 05:46:41,284 - train - INFO - Training Started : n_epoch=30\n",
      "2022-12-08 05:46:41,415 - train - INFO -  * In epoch 0001, loss=0.693, acc=0.488, AUC=0.495\n",
      "2022-12-08 05:46:41,416 - train - INFO -  * In epoch 0001, loss=0.693, acc=0.488, AUC=0.495, Best AUC\n",
      "2022-12-08 05:46:41,532 - train - INFO -  * In epoch 0002, loss=0.693, acc=0.490, AUC=0.497\n",
      "2022-12-08 05:46:41,533 - train - INFO -  * In epoch 0002, loss=0.693, acc=0.490, AUC=0.497, Best AUC\n",
      "2022-12-08 05:46:41,673 - train - INFO -  * In epoch 0003, loss=0.693, acc=0.490, AUC=0.498\n",
      "2022-12-08 05:46:41,674 - train - INFO -  * In epoch 0003, loss=0.693, acc=0.490, AUC=0.498, Best AUC\n",
      "2022-12-08 05:46:41,809 - train - INFO -  * In epoch 0004, loss=0.693, acc=0.489, AUC=0.500\n",
      "2022-12-08 05:46:41,810 - train - INFO -  * In epoch 0004, loss=0.693, acc=0.489, AUC=0.500, Best AUC\n",
      "2022-12-08 05:46:41,940 - train - INFO -  * In epoch 0005, loss=0.693, acc=0.488, AUC=0.501\n",
      "2022-12-08 05:46:41,941 - train - INFO -  * In epoch 0005, loss=0.693, acc=0.488, AUC=0.501, Best AUC\n",
      "2022-12-08 05:46:42,076 - train - INFO -  * In epoch 0006, loss=0.693, acc=0.487, AUC=0.503\n",
      "2022-12-08 05:46:42,077 - train - INFO -  * In epoch 0006, loss=0.693, acc=0.487, AUC=0.503, Best AUC\n",
      "2022-12-08 05:46:42,218 - train - INFO -  * In epoch 0007, loss=0.693, acc=0.491, AUC=0.505\n",
      "2022-12-08 05:46:42,219 - train - INFO -  * In epoch 0007, loss=0.693, acc=0.491, AUC=0.505, Best AUC\n",
      "2022-12-08 05:46:42,347 - train - INFO -  * In epoch 0008, loss=0.693, acc=0.492, AUC=0.506\n",
      "2022-12-08 05:46:42,349 - train - INFO -  * In epoch 0008, loss=0.693, acc=0.492, AUC=0.506, Best AUC\n",
      "2022-12-08 05:46:42,480 - train - INFO -  * In epoch 0009, loss=0.693, acc=0.494, AUC=0.508\n",
      "2022-12-08 05:46:42,481 - train - INFO -  * In epoch 0009, loss=0.693, acc=0.494, AUC=0.508, Best AUC\n",
      "2022-12-08 05:46:42,610 - train - INFO -  * In epoch 0010, loss=0.693, acc=0.494, AUC=0.509\n",
      "2022-12-08 05:46:42,611 - train - INFO -  * In epoch 0010, loss=0.693, acc=0.494, AUC=0.509, Best AUC\n",
      "2022-12-08 05:46:42,749 - train - INFO -  * In epoch 0011, loss=0.693, acc=0.496, AUC=0.511\n",
      "2022-12-08 05:46:42,750 - train - INFO -  * In epoch 0011, loss=0.693, acc=0.496, AUC=0.511, Best AUC\n",
      "2022-12-08 05:46:42,878 - train - INFO -  * In epoch 0012, loss=0.693, acc=0.498, AUC=0.512\n",
      "2022-12-08 05:46:42,879 - train - INFO -  * In epoch 0012, loss=0.693, acc=0.498, AUC=0.512, Best AUC\n",
      "2022-12-08 05:46:43,014 - train - INFO -  * In epoch 0013, loss=0.693, acc=0.496, AUC=0.514\n",
      "2022-12-08 05:46:43,015 - train - INFO -  * In epoch 0013, loss=0.693, acc=0.496, AUC=0.514, Best AUC\n",
      "2022-12-08 05:46:43,115 - train - INFO -  * In epoch 0014, loss=0.693, acc=0.497, AUC=0.515\n",
      "2022-12-08 05:46:43,116 - train - INFO -  * In epoch 0014, loss=0.693, acc=0.497, AUC=0.515, Best AUC\n",
      "2022-12-08 05:46:43,252 - train - INFO -  * In epoch 0015, loss=0.693, acc=0.497, AUC=0.517\n",
      "2022-12-08 05:46:43,253 - train - INFO -  * In epoch 0015, loss=0.693, acc=0.497, AUC=0.517, Best AUC\n",
      "2022-12-08 05:46:43,379 - train - INFO -  * In epoch 0016, loss=0.693, acc=0.498, AUC=0.519\n",
      "2022-12-08 05:46:43,380 - train - INFO -  * In epoch 0016, loss=0.693, acc=0.498, AUC=0.519, Best AUC\n",
      "2022-12-08 05:46:43,502 - train - INFO -  * In epoch 0017, loss=0.693, acc=0.498, AUC=0.520\n",
      "2022-12-08 05:46:43,503 - train - INFO -  * In epoch 0017, loss=0.693, acc=0.498, AUC=0.520, Best AUC\n",
      "2022-12-08 05:46:43,620 - train - INFO -  * In epoch 0018, loss=0.693, acc=0.502, AUC=0.522\n",
      "2022-12-08 05:46:43,622 - train - INFO -  * In epoch 0018, loss=0.693, acc=0.502, AUC=0.522, Best AUC\n",
      "2022-12-08 05:46:43,757 - train - INFO -  * In epoch 0019, loss=0.693, acc=0.504, AUC=0.524\n",
      "2022-12-08 05:46:43,758 - train - INFO -  * In epoch 0019, loss=0.693, acc=0.504, AUC=0.524, Best AUC\n",
      "2022-12-08 05:46:43,884 - train - INFO -  * In epoch 0020, loss=0.693, acc=0.504, AUC=0.525\n",
      "2022-12-08 05:46:43,886 - train - INFO -  * In epoch 0020, loss=0.693, acc=0.504, AUC=0.525, Best AUC\n",
      "2022-12-08 05:46:44,023 - train - INFO -  * In epoch 0021, loss=0.693, acc=0.505, AUC=0.527\n",
      "2022-12-08 05:46:44,024 - train - INFO -  * In epoch 0021, loss=0.693, acc=0.505, AUC=0.527, Best AUC\n",
      "2022-12-08 05:46:44,151 - train - INFO -  * In epoch 0022, loss=0.693, acc=0.506, AUC=0.528\n",
      "2022-12-08 05:46:44,152 - train - INFO -  * In epoch 0022, loss=0.693, acc=0.506, AUC=0.528, Best AUC\n",
      "2022-12-08 05:46:44,292 - train - INFO -  * In epoch 0023, loss=0.693, acc=0.509, AUC=0.530\n",
      "2022-12-08 05:46:44,293 - train - INFO -  * In epoch 0023, loss=0.693, acc=0.509, AUC=0.530, Best AUC\n",
      "2022-12-08 05:46:44,422 - train - INFO -  * In epoch 0024, loss=0.693, acc=0.513, AUC=0.532\n",
      "2022-12-08 05:46:44,423 - train - INFO -  * In epoch 0024, loss=0.693, acc=0.513, AUC=0.532, Best AUC\n",
      "2022-12-08 05:46:44,559 - train - INFO -  * In epoch 0025, loss=0.693, acc=0.514, AUC=0.534\n",
      "2022-12-08 05:46:44,560 - train - INFO -  * In epoch 0025, loss=0.693, acc=0.514, AUC=0.534, Best AUC\n",
      "2022-12-08 05:46:44,687 - train - INFO -  * In epoch 0026, loss=0.693, acc=0.513, AUC=0.536\n",
      "2022-12-08 05:46:44,688 - train - INFO -  * In epoch 0026, loss=0.693, acc=0.513, AUC=0.536, Best AUC\n",
      "2022-12-08 05:46:44,828 - train - INFO -  * In epoch 0027, loss=0.693, acc=0.513, AUC=0.537\n",
      "2022-12-08 05:46:44,829 - train - INFO -  * In epoch 0027, loss=0.693, acc=0.513, AUC=0.537, Best AUC\n",
      "2022-12-08 05:46:44,948 - train - INFO -  * In epoch 0028, loss=0.693, acc=0.518, AUC=0.539\n",
      "2022-12-08 05:46:44,949 - train - INFO -  * In epoch 0028, loss=0.693, acc=0.518, AUC=0.539, Best AUC\n",
      "2022-12-08 05:46:45,077 - train - INFO -  * In epoch 0029, loss=0.693, acc=0.519, AUC=0.541\n",
      "2022-12-08 05:46:45,079 - train - INFO -  * In epoch 0029, loss=0.693, acc=0.519, AUC=0.541, Best AUC\n",
      "2022-12-08 05:46:45,213 - train - INFO -  * In epoch 0030, loss=0.693, acc=0.519, AUC=0.543\n",
      "2022-12-08 05:46:45,214 - train - INFO -  * In epoch 0030, loss=0.693, acc=0.519, AUC=0.543, Best AUC\n",
      "2022-12-08 05:46:45,215 - train - INFO - Best Weight Confirmed : 30'th epoch\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_data,\n",
    "    n_epoch=CFG.n_epoch,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    weight=CFG.weight_basepath,\n",
    "    logger=logger.getChild(\"train\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data, logger=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.predict_link(data[\"edge\"], prob=True)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npred = inference(model, test_data, logger=logger.getChild(\"infer\"))\\npred = pred.detach().cpu().numpy()\\npd.DataFrame({\"prediction\": pred}).to_csv(\\n    os.path.join(CFG.output_dir, CFG.pred_file), index_label=\"id\"\\n)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pred = inference(model, test_data, logger=logger.getChild(\"infer\"))\n",
    "pred = pred.detach().cpu().numpy()\n",
    "pd.DataFrame({\"prediction\": pred}).to_csv(\n",
    "    os.path.join(CFG.output_dir, CFG.pred_file), index_label=\"id\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edge_index를 인풋으로 받음: (2, k) 형태, device 통일\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_embedding(self, edge_index: Adj) -> Tensor:\\n        x = self.embedding.weight\\n        out = x * self.alpha[0]\\n\\n        for i in range(self.num_layers):\\n            x = self.convs[i](x, edge_index)\\n            out = out + x * self.alpha[i + 1]\\n\\n        return out\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_embedding(self, edge_index: Adj) -> Tensor:\n",
    "        x = self.embedding.weight\n",
    "        out = x * self.alpha[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "\n",
    "        return out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_embedding_train = model.get_embedding(train_data[\"edge\"])\n",
    "gcn_embedding_test = model.get_embedding(test_data[\"edge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "a = torch.randint(0, 7442, size=(1, k)).to(device)  # 7442\n",
    "b = torch.randint(7442, 7442+9454, size=(1, k)).to(device)  # 7442+9454\n",
    "c = torch.cat([a,b], dim=0)\n",
    "gcn_embedding_c = model.get_embedding(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.randint(7442+9454, size=(2, k)).to(device)\n",
    "gcn_embedding_d = model.get_embedding(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.randint(7442+9454, size=(2, 1)).to(device)\n",
    "gcn_embedding_e = model.get_embedding(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embedding.weight: True \n",
      "gcn_embedding_train: False \n",
      "gcn_embedding_test: False\n"
     ]
    }
   ],
   "source": [
    "print(\"model.embedding.weight:\", False in (model.embedding.weight == gcn_embedding_c), \"\\n\"\n",
    "    \"gcn_embedding_train:\", False in (gcn_embedding_train == gcn_embedding_c), \"\\n\" \n",
    "    \"gcn_embedding_test:\", False in (gcn_embedding_test == gcn_embedding_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embedding.weight: True \n",
      "gcn_embedding_train: False \n",
      "gcn_embedding_test: False\n"
     ]
    }
   ],
   "source": [
    "print(\"model.embedding.weight:\", False in (model.embedding.weight == gcn_embedding_d), \"\\n\"\n",
    "    \"gcn_embedding_train:\", False in (gcn_embedding_train == gcn_embedding_d), \"\\n\" \n",
    "    \"gcn_embedding_test:\", False in (gcn_embedding_test == gcn_embedding_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embedding.weight: True \n",
      "gcn_embedding_train: False \n",
      "gcn_embedding_test: False\n"
     ]
    }
   ],
   "source": [
    "print(\"model.embedding.weight:\", False in (model.embedding.weight == gcn_embedding_e), \"\\n\"\n",
    "    \"gcn_embedding_train:\", False in (gcn_embedding_train == gcn_embedding_e), \"\\n\" \n",
    "    \"gcn_embedding_test:\", False in (gcn_embedding_test == gcn_embedding_e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('gcn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e20694c0580b936adf946462b21f5f5ff4a6066458c55bf72c7dd0f60ef410ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
